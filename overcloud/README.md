Chapter 6: Deploy OpenStack
===========================

We can now finally deploy OpenStack on our infrastructure:

    overcloud/deploy

Deployment is handled by the `openstack overcloud deploy` command. It is configured by the files in
`configuration/overcloud/`.

During installation each infrastructure node is assigned a "role" and is provisioned with an
operating system image via PXE (which we prepared in step 1). After the operating systems boots, the
process continues similarly to how it worked with TripleO (the undercloud) in the previous chapter.
The main difference is that our undercloud is an all-in-one OpenStack installation while our
overcloud has different sets of OpenStack services deployed per machine according to its role. So,
let's repeat what we said about the TripleO deployment: 

Internally it will be using [Puppet](https://puppet.com/) to orchestrate the installation and
configuration of the various OpenStack services as containers to be run by
[Podman](https://podman.io/). Containers allow for better isolation and portability. The undercloud
itself is installed using the OpenStack [Heat](https://docs.openstack.org/heat/latest/)
orchestration and [Mistral](https://docs.openstack.org/mistral/latest/) workflow services, which
internally use [Ansible](https://www.ansible.com/). The OpenStack container images are provided by
the [Kolla project](https://docs.openstack.org/kolla/latest/).

Our OpenStack storage is handled by [Ceph](https://ceph.com/) and its deployment is handled by yet
another project, [Ceph-Ansible](http://docs.ceph.com/ceph-ansible/), which provides its own
container images.

> In production deployments we would likely have dedicated storage machines, indeed different kinds
according to the various Ceph roles. But that is unnecessary for our lab, where it would be fine to
have our single machine provide both compute and storage resources. Thus we will be deploying a
[hyper-converged infrastructure (HCI)](https://en.wikipedia.org/wiki/Hyper-converged_infrastructure)
version of the "Compute" role, named "ComputeHCI". We will even be fine-tuning Ceph for optimal
performance for this converged setup.  

TODO: explain networking, OVS, OVN

All off this takes a while. Expect TODO 

As in the previous step, it could also be useful to see the changing state of the infrastructure
nodes. We can set up a `watch` like so:

    watch manager/undercloud/openstack baremetal node list

We should see the nodes getting an "Instance UUID" and then changing their provisioning
state from "available" to "deploying". They should then power on and change to "wait call-back".
As they start to PXE boot the state will go back to "deploying" and finally "active" when they are
fully installed.

At this point the overcloud servers will also be up. We can simultaneously set up yet another watch
for them:

    watch manager/undercloud/openstack server list

Their initial status will be "BUILD" and eventually switch to "ACTIVE" and they will receive network
IP addresses.

Note that the server names are *not* the same as the infrastructure node names: this is where the
difference between undercloud and overcloud becomes clear. The server names comprise the Heat stack
name ("lab"), the role name, and a sequenced ID beginning with "0". So, our first and only
controller would be `lab-controller-0`, and storage-converged compute node would be
`lab-computehci-0`.

After deployment is done we will find some useful files in the `tripleo` virtual machine at the
`tripleo` user's home directory: 

* `/home/tripleo/overcloud-container-images.yaml` was generated by us calling
   `openstack tripleo container image prepare`
* `/home/tripleo/heat-environments.yaml` copied from
  `configuration/overcloud/heat-environments.yaml` and referencing the above file

*

Logs and configuration files have been fetched to the `workspace/results/` directory. Some useful
ones are from:

`/var/lib/mistral/lab/ansible.log`

`/var/lib/mistral/lab/ceph-ansible/ceph_ansible_command.log`

* TripleO: `/var/log/containers/ironic/deploy/` contains `tar.gz` files for each infrastructure node
  workflow, which internally contain the `journal` of the installation process


TODO tail this file:
/var/lib/mistral/overcloud/ansible.log

manager/undercloud/openstack baremetal node undeploy compute-0

openstack/openstack user list

openstack/openstack stack failures list

The `openstack` command is documented
[here](https://docs.openstack.org/python-openstackclient/stein/cli/).


MISSING PIECES FROM ABAYS

tripleo:
sudo ip r add 10.0.0.0/24 dev br-ctlplane

controller:
sudo ovs-vsctl remove port vlan10 tag 10

manager:
sudo ip a add 10.0.0.69/24 dev openstack


https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/8/html/ipv6_networking_for_the_overcloud/configuring_the_overcloud_before_creation

tripleo
sudo ovs-vsctl add-port br-ctlplane vlan10 tag=10 -- set interface vlan10 type=internal
sudo ip l set dev vlan10 up
...
sudo ovs-vsctl del-port vlan10


in `/var/lib/mistral/lab/ceph-ansible/ceph_ansible_command.log`
podman pull failed


openstack compute service list



controller:
sudo ovs-vsctl add-port br-public eth2
sudo ip addr add 10.0.2.2 dev br-public



	#openstack stack resource list overcloud
	#openstack stack resource show overcloud ControllerServiceChain



sudo tcpdump -i br-ex -nn -e vlan | grep "vlan 50"






Installing the Cloud
====================

If we're installing an all-in-one setup, then we should already have the cloud virtual machines set
up for us from the previous step: `controller-0`, `compute-0`, and `ceph-0`. In RDO, each cloud
machine is assigned a role that defines which OpenStack components run on it:

* Controller:
  Manages networking ([Neutron](https://docs.openstack.org/neutron/latest/)) as well
  as virtual machine scheduling
  ([nova-scheduler](https://docs.openstack.org/nova/latest/cli/nova-scheduler.html)).
  It requires only minimal compute and disk resources. The defaults should suffice.
* Compute: [nova-compute](https://docs.openstack.org/nova/latest/cli/nova-compute.html). This is
  where our cloud virtual machines will be provisioned (in nested virtualization). So, we want this
  machine to be alloted a lot of RAM and CPU cores.
* Ceph: [Ceph](https://ceph.com/) is a powerful distributed storage system with good OpenStack
  integration. This is where our
  block storage ([Cinder](https://docs.openstack.org/cinder/latest/)),
  file storage ([Manila](https://docs.openstack.org/manila/latest/)),
  and object storage ([Swift](https://docs.openstack.org/swift/latest/)) will be provisioned.
  So, on the lab we want this machine to be alloted a lot of disk space. RAM and CPU defaults
  should suffice.

> One interesting implementation detail: if you remember, our cloud manager uses OpenStack's
[Ironic](https://wiki.openstack.org/wiki/Ironic) component to manage the cloud machines. You might
think that it only uses Ironic for bare metal machines, which is what it was designed for. In the
all-in-one setup we are using virtual machines, we shouldn't have to use Ironic, right?
In fact, we are using Ironic for all machines, whether bare metal or virtual. We do this in order to
maintain a single management path for all machines. This works well, but it does involve some
cunning to make the virtual machines behave more like bare metal. To learn more about this
technology see [VirtualBMC](https://github.com/openstack/virtualbmc), which exposes an
[IPMI](https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface).



How to Reset
------------

Note that we cannot do this with the `openstack-undercloud` shortcut, because we need elevated
credentials in order to access the overcloud.

    ./ssh-virtual stack@undercloud-0
    . stackrc
    openstack overcloud delete overcloud

Next
----

[Continue to Chapter 7: Example Workloads](../workloads/README.md)
