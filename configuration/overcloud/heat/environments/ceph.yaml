# Ceph configuration.
#
# This is a Heat template environment.
#
# See:
#   https://docs.openstack.org/project-deploy-guide/tripleo-docs/latest/features/ceph_config.html
#   https://docs.openstack.org/heat/train/template_guide/environment.html

parameter_defaults:

  CephAnsiblePlaybookVerbosity: 1 # the default is 0

  # Pool
  # ----

  # The default pool size of 3 can cause an error during deployment:
  # "pg_num 128 size 3 would mean 768 total pgs, which exceeds max 750"
  # The basic issue is that we cannot have a pool size bigger than the total amount of storage nodes
  # (our lab has only 1 node)
  CephPoolDefaultSize: 1

  # But even if deployment succeeds, we might still get a placement group warning:
  # "too many PGs per OSD (320 > max 250)" when running "ceph status"
  # (Note that that the Ceph runtime max is different from the Ceph Ansible max used above)
  # See: https://bugs.launchpad.net/kolla-ansible/+bug/1763356
  #      https://docs.ceph.com/docs/mimic/rados/operations/placement-groups/
  # To see the pg_num value at runtime: "ceph osd pool get images pg_num"
  CephPoolDefaultPgNum: 16 # the default is 128

  # Disks
  # -----

  # The default is to use disk "/dev/vdb", which would work for a virtual machine,
  # But for a physical machine we would need to explicitly set the disks
  CephAnsibleDisksConfig:
    devices:
    - /dev/sdb
    #osd_scenario: collocated # to collocate on a single drive

  # Hyperconvergence
  # ----------------

  # See note here under "ceph-ansible 3.2 and newer":
  # https://docs.openstack.org/project-deploy-guide/tripleo-docs/latest/features/ceph_config.html
  CephAnsibleExtraConfig:
    is_hci: true
